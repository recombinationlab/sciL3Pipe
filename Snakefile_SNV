
import glob
from os import path
import datetime
import yaml
from collections import defaultdict
import sys
import shutil
import pandas as pd

################################################################################
# Load samples 
################################################################################

INFO = ''

ALL_SAMPLES = []
FILES = defaultdict(list)
OUTPUT_DIR = defaultdict(str)
# SAMPLES_IN_DIR = defaultdict(list)
for i in range(len(config["input_folder"])):
    FULL_PATH = glob.glob(os.path.join(config["input_folder"][i], "*.bam"))
    for f in FULL_PATH:
        sample = os.path.basename(f).split('.bam')[0]
        ALL_SAMPLES.append(sample)
        FILES[sample].append(f)
        OUTPUT_DIR[sample] = config["output_folder"][i]
        # SAMPLES_IN_DIR[config["output_folder"][i]].append(sample)

# INFO += f'Samples to process {ALL_SAMPLES} \n'

try:
    bam_filter="-f " + config["hybrid_filter"]
except:
    bam_filter=""

try:
    prefix_1 = config["prefix_1"]
    prefix_2 = config["prefix_2"]
    process_only = config["process_only"]
except:
    sys.exit("ERROR: prefix or process only parameters not set in config")

try:
    lianti_path = config["lianti_path"]
    assert shutil.which(lianti_path) is not None, "lianti not executable or does not exist"
except:
    sys.exit("lianti not found")

try:
    snpsift_path = config["snpsift_path"]
except:
    sys.exit("SnpSift.jar not in config file")

assert shutil.which(snpsift_path) is not None, "SnpSift.jar not executable or does not exist"

try:
    qc_folder = config["qc_folder"]
except:
    sys.exit("Specify output folder for QC file")


try:
    filt_only = config['filter_only']
    INFO += 'Only filtering'
except:
    INFO += 'Calling SNVs'

# SPLIT = [OUTPUT_DIR[s] + f"logs/{s}_hybrid_split.log" for s in ALL_SAMPLES]

# VCF = [OUTPUT_DIR[s] + f"{process_only}/{s}.{process_only}.filtered.vcf.gz" for s in ALL_SAMPLES]
# QC = [f"{d}logs/qc.tsv" for d in SAMPLES_IN_DIR.keys()]

# VCF = [OUTPUT_DIR[s] + f"{s}.{prefix_1}.bam" for s in ALL_SAMPLES] + [OUTPUT_DIR[s] + f"{s}.{prefix_2}.bam" for s in ALL_SAMPLES]

# FILT = [OUTPUT_DIR[s] + f"{process_only}/{{s}}.{process_only}.filt.bam" for s in ALL_SAMPLES]



def get_results(wildcards):
    print(checkpoints.qc.get().output[0])
    qc = pd.read_csv(checkpoints.qc.get().output[0], sep="\t")

    # BAM has fewer than 30% of total reads or less than 10000 reads and will be excluded
    if process_only == 'mouse': # BAM_1
        samples = qc[(pd.to_numeric(qc['BAM_2_PofT']) < 30) & (pd.to_numeric(qc['Total']) > 10000)]['Files']
    elif process_only == 'human': # BAM_2
        samples = qc[(pd.to_numeric(qc['BAM_1_PofT']) < 30) & (pd.to_numeric(qc['Total']) > 10000)]['Files']
    
    files_out = []

    for s in samples:
        s = os.path.basename(s).split('_hybrid_split.log')[0]
        if filt_only:
            files_out.extend([OUTPUT_DIR[s] + f"{process_only}/{s}.{process_only}.filt.bam"])
            files_out.extend([qc_folder + "logs/idxstats_summary.tsv"])
        else:
            files_out.extend([OUTPUT_DIR[s] + f"{process_only}/{s}.{process_only}.filtered.vcf.gz"])
            files_out.extend([qc_folder + "logs/idxstats_summary.tsv"])
    return files_out


def log_parse(log):
    '''
    Parse split_hybrid.py log file(s).
    '''
    linecount = 0
    name, bam_1, bam_2  = [None] * 3
    for line in log:
        linecount += 1

        if line.rstrip().endswith('log'):
            name = line.rstrip()
        elif line.startswith('BAM 1'):
            bam_1 = line.rstrip().split(':')[-1]
        elif line.startswith('BAM 2'):
            bam_2 = line.rstrip().split(':')[-1]
            yield name, bam_1, bam_2
            name, bam_1, bam_2  = [None] * 3
        else:
            continue
            





################################################################################
################################################################################
# Execute before workflow starts
################################################################################
################################################################################
onstart:
    print(INFO)
################################################################################
################################################################################
# Rule all
################################################################################
################################################################################
rule all:
    input: 
        get_results
        # QC 
        # VCF
    
wildcard_constraints:
    output_dir="^.*(?:(\/))"
    # sample="([^/]+$)"
################################################################################
# QC and filtering
################################################################################

rule split_hybrid:
    '''
    Assume the BAM files are coordinate sorted
    '''
    input:
        lambda wildcards: FILES[os.path.basename(wildcards.sample)]
    output:
        f"{{output_dir}}{prefix_1}/{{sample}}.{prefix_1}.bam",
        f"{{output_dir}}{prefix_2}/{{sample}}.{prefix_2}.bam",
        f"{{output_dir}}logs/{{sample}}_hybrid_split.log"
    conda:
        "envs/sciStrand_env.yaml"
    # log:
    #     f"{{output_dir}}logs/{{sample}}_hybrid_split.log"
    shell:
        '''
        samtools index {input}

        python scripts/split_hybrid.py \
        -i {input} \
        -o {wildcards.output_dir} \
        -1 {prefix_1} \
        -2 {prefix_2} \
        {bam_filter} &> {output[2]}
        '''


checkpoint qc:
    '''
    Only output subset of BAM files, based on criteria in get_results
    '''
    input:
        [OUTPUT_DIR[s] + f"logs/{s}_hybrid_split.log" for s in ALL_SAMPLES]
    output:
        qc_folder + "logs/qc.tsv"
    run:
        rows = []
        for f in input:
            with open(f) as tsv:
                for name, bam_1, bam_2 in log_parse(tsv):
                    total = int(bam_1) + int(bam_2)
                    bam_1_pot = (int(bam_1)/total)*100
                    bam_2_pot = (int(bam_2)/total)*100
                    rows.append([f, bam_1, bam_2, bam_1_pot, bam_2_pot, total])

        df = pd.DataFrame(rows, columns=['Files', 'BAM_1', 'BAM_2', 'BAM_1_PofT', 'BAM_2_PofT', 'Total'])
        df.to_csv(output[0], sep='\t', index=False)



rule filter_bam:
    '''
    Filter excessive soft-clipping
    '''
    input:
        f"{{output_dir}}{process_only}/{{sample}}.{process_only}.bam"
    output:
        f"{{output_dir}}{process_only}/{{sample}}.{process_only}.filt.bam"
    conda:
        "envs/sciStrand_env.yaml"
    log:
        f"{{output_dir}}logs/{{sample}}_filtered.log"
    params:
        dup='--mark_duplicate' if config['mark_duplicate'] else ''
    threads:
        5
    shell:
        '''
        python scripts/filter_bam.py \
        -i {input} \
        -o {output} \
        --paired \
        --threads {threads} \
        --insert_max 2000 \
        {params.dup} \
        --sort \
        --no_plot \
        --max_ratio 0.5 &> {log}
        '''


rule idxstats_qc:
    '''
    Summary of idxstats for all BAM files
    '''
    input:
        [OUTPUT_DIR[s] + f"{process_only}/{s}.{process_only}.bam" for s in ALL_SAMPLES]
    output:
        qc_folder + "logs/idxstats_summary.tsv"
    params:
        dir = ' '.join(config["input_folder"]),
        format = 'ensembl' if process_only == 'human' else 'UCSC'
    shell:
        '''
        echo {params.dir}
        python scripts/aggregate_idxstats.py \
        -d {params.dir} \
        -o {output} \
        --format {params.format} \
        --regex "*.bam"
        '''


################################################################################
# SNV calling and annotation
################################################################################

rule lianti_pileup:
    '''
    tabix for SnpSift
    only keep annotated files

    -v/V, --types/--exclude-types <list>        select/exclude comma-separated list of variant types: snps,indels,mnps,ref,bnd,other [null]
    -m/M, --min-alleles/--max-alleles <int>     minimum/maximum number of alleles listed in REF and ALT (e.g. -m2 -M2 for biallelic sites)
    -O, --output-type <b|u|z|v>                 b: compressed BCF, u: uncompressed BCF, z: compressed VCF, v: uncompressed VCF [v]
    '''
    input:
        # f"{{output_dir}}{prefix_1}/{{sample}}.{prefix_1}.bam",
        # f"{{output_dir}}{prefix_2}/{{sample}}.{prefix_2}.bam"
        f"{{output_dir}}{process_only}/{{sample}}.{process_only}.filt.bam"
    output:
        vcf=temp(f"{{output_dir}}{process_only}/{{sample}}.{process_only}.vcf.gz"),
        tbi=temp(f"{{output_dir}}{process_only}/{{sample}}.{process_only}.vcf.gz.tbi")
    conda:
        "envs/sciStrand_env.yaml"
    log:
        f"{{output_dir}}logs/{{sample}}_lianti_pileup.log"
    params:
        fa_ref=config["ref_fa"]
    shell:
        '''
        {lianti_path} pileup \
        -cf {params.fa_ref} \
        -C -q1,1 -Q20,20 -s2 -P20 -L 1 \
        {input} |
        grep -v "0,0:0,0:0$" | \
        bcftools view -v snps -m 2 -M 2 -O z -o {output.vcf} &> {log}

        tabix -p vcf {output.vcf}
        '''

rule vcf_annotation:
    input:
        vcf=f"{{output_dir}}{process_only}/{{sample}}.{process_only}.vcf.gz",
        tbi=f"{{output_dir}}{process_only}/{{sample}}.{process_only}.vcf.gz.tbi"
    output:
        f"{{output_dir}}{process_only}/{{sample}}.{process_only}.filtered.vcf.gz"
    conda:
        "envs/sciStrand_env.yaml"
    params:
        dbsnp=config["dbsnp"]
    shell:
        '''
        java -Xmx5G -Xms5G -jar {snpsift_path} annotate -id -noInfo  \
        {params.dbsnp} \
        {input.vcf} | \
        bgzip > {output}
        '''
